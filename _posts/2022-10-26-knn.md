---
title: \[ AIB ] Clustering(군집화)
author: yun
date: 2022-10-26
categories: [Blogging, Study, Ai, Summary, Pca]
tags: [study, python, covariance, pca]
---


머신러닝의 종류
1. Supervised (지도학습) : target이 있습니다.
2. Unsupervised (비지도학습) : target이 없다. target column이 존재하지 않습니다. 
 <br/>

## 군집화란 무엇인가요?
* 서로 유사한 데이터들을 같은 그룹으로, 서로 유사하지 않은 데이터는 다른 그룹으로 분리하는 것. <br/>
> 몇 개의 그룹을 묶을 것인지?
> 데이터의 유사도를 어떻게 정의할 것인지?

* 위의 두 질문을 해결할 수 있는 전략은 **`K-Means`** 알고리즘 입니다. (**K-Means Clustring** 알고리즘)
 <br/>

## K-Means Clustring
***
### K-Means 군집화란?
가장 대표적인 클러스터링 알고리즘 중의 하나로 **Centroid Based Clustering** 알고리즘 이라고도 합니다. <br/>
**Centroid(군집 중심점)** 이란, 주어진 클러스터 내부에 있는 **모든 점들의 중심 부분에 위치한 점**을 의미합니다. <br/>
K-Means 군집화는 Centroid라는 특정한 임의의 지점을 선택해 해당 중심점으로부터 *거리가 가장 가까운* 포인트를 같은 클러스터 <br/>
즉, *비슷한 특성을 가진 데이터들이 모인 집단으로 묶는 방법*입니다. <br/> 
데이터가 비슷하다는 것은 **데이터 사이간의 거리가 가까움**을 의미합니다. <br/>
따라서 *K-Means 군집화*는 데이터들 사이의 거리를 측정한 뒤 그 거리를 기반으로 가가운 데이터들끼리 클러스터로 묶어줍니다.
> 여기서 질문. 데이터들 사이의 거리는 어떻게 측정할 것인가?<br/>

* 데이터 사이의 거리 계산 <br/>
  * Euclidean Distance
  * Cosice Similarity
  * Jaccard Distance

어떤 거리 계산 방식을 가장 많이 쓰냐 하면, <br/>
진리의 케이스 바이 케이스이긴 하지만 일반적으로 많이 쓰이는 방식은 **Euclidean Distance** 입니다. 

### **K-Means 군집화의 절차**
 * **Euclidean Distance** 계산법을 통해 데이터 사이들의 거리를 계산하고 가까운 데이터들 끼리 묶어주게 됩니다.
 * 데이터들을 묶는 과정은 3가지 단계를 거치게 됩니다. 
 * 아래는 Centroid를 2개로 지정(K=2)했을 때의 절차입니다.<br/>
  1. K=2 개의 임의의 중심점을 배치한다.
  2. 각 데이터들을가장 가가운 중심점으로 할당한다.
  3. 군집으로 지정된 데이터들을 기반으로 해당 군집의 중심점을 업데이트 한다.
  4. 2번, 3번 단계를 최종적으로 중심점이 수렴될 때까지 즉, 더이상  중심점이 업데이트 되지 않을 때까지 반복한다.

> 주목해야할 점은, K개의 중심점을 가지고 분류를 하기에 사용자가 지정한 K값이 결과에 영향을 미칠 수 있다는 점입니다. <br/>
> 따라서 **최적의 K 즉, 적절한 클러스터의 개수를 찾는 방법**을 살펴봅시다. <br/>

### Elbow Method
데이터의 차원수와 갯수가 많아질수록 사람의 육안으로 데이터를 몇 개의 그룹으로 군집화 해야겠다고 결정하는 것은 쉽지 않습니다. <br/>
그렇기 때문에 최대한 수학적으로 몇 개의 그룹으로 데이터를 묶어주는 것이 좋은지 계산해야 합니다. 이것이 **Elbow Method**입니다. <br/>
<br/>
Elbow Method는 K-Means 군집화 알고리즘의 성능을 최대화 시키는 적합한 K의 개수를 선택하는 방법을 제공합니다. <br/>
그 방법은 *Emphirical method*로서 다양한 K값을 이용해 데이터를 군집화 한 뒤, 각 K값에 해당하는 군집 내의 데이터들이 <br/>
얼마나 퍼져있거나 뭉쳐있는지 **응집도**, 즉 `inertia`를 값으로 확인합니다.  <br/>
inertia의 최솟값을 찾는 것이 아니라 그 **기울기**를 봅니다. <br/>
> `inertia`는 *각 클러스터 별 오차의 제곱의 합(분산)*을 나타냅니다. 각 데이터로부터 자신이 속한 군집의 중심까지의 거리를 의미합니다. <br/>
> 그러므로 **inertia 값이 낮을수록(기울기가 낮을수록) 군집화가 더 잘됐다**고 볼 수 있습니다. <br/>

## EDA
***
### Data Description










