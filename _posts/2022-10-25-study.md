---
title: \[ AIB ] 공분산, 상관계수, PCA
author: yun
date: 2022-10-25
categories: [Blogging, Study, Ai, Summary, Pca]
tags: [study, python, covariance, pca]
---

# 차원축소 PCA
*** 
<br/>
두 변수의 공변 (covariant) <br/>
벡터 변환 (vector transformation) <br/>
벡터의 투영 (projection) <br/>

## 1. 공분산(Covariance)과 상관계수(Correlation coefficient)
### 분산 (Varicance)은 무엇입니까?
 - 데이터가 흩어져 있는 정도를 하나의 값으로 나타낸 것입니다. <br/>
 - 데이터가 서로 멀리 떨어져 있을 수록 분산의 값이 커집니다. <br/>
 - 편차 제곱의 평균으로 구합니다. <br/>
 - > $\sigma^2 = \frac{\sum{(X_{i} - \overline{X})^{2}} }{N} \quad$ where $\ $ $X_i$:관측값 , $\bar{X}$:평균 , $N$: 관측값 개수 <br/>
 - 편차 = (관측값) - (평균) <br/>
 - 편차의 합은 항상 0이기 때문에 편차의 평균도 항상 0입니다. <br/>
 - df.var() 또는 np.var() 를 사용하여 구할 수 있다. 
 <br/><br/>
 
### 표준편차는 무엇입니까?
 - 분산의 제곱근($\sqrt{ﾠ}$) 값입니다. <br/>
 - 부산의 스케일을 조정하는 효과가 있습니다. <br/>
 - df.std() 또는 np.std()를 사용하여 구할 수 있습니다. <br/>
 <br/>
 
 > ddof 파라미터는 무엇일까요?

### 공분산 (共分散, Covariance)은 무엇입니까?
- 함께라는 의미의 '共(공)' <br/>
- 두 변수에 대하여 한 변수가 변화할 때 다른 변수가 어떠한 연관성을 갖고 변하는지를 나타낸 값입니다. <br/>
- 두 변수의 연관성이 클수록 공분산 값도 커집니다. <br/>
- 공분산 값이 0이라고 해서 두 변수가 항상 연관성이 없다고 단정지을 수 **`없다`** <br/>
- (이미지 첨부할 것) <br/>

> 두 변수의 스케일이 크다면 공분산 값은 어떻게 될까요? 

```python
# 5부터 50까지의 수 중, 값이 5씩 증가하는 수
a1 = a2 = np.arange(5, 50, 5)
# 10부터 100까지의 수 중, 값이 10씩 증가하는 수
a3 = a4 = np.arange(10, 100, 10)
 
data = {"a1": a1, "a2": a2, "a3": a3, "a4": a4}

df = pd.DataFrame(data)
df
```

### 분산-공분산 행렬 (Variance-covariace matrix)
* 모든 변수에 대해 분산과 공분산 값을 나타내는 정사각 행렬입니다. <br/>
  * 주 대각선 성분은 자기 자신의 분산 값을 나타냅니다. <br/>
  * 주 대각선 이외의 성분은 가능한 두 변수의 공분산 값을 나타냅니다. <br/>
* **`df.cov()`** 또는 **`np.cov()`** 를 사용하여 구할 수 있습니다. <br/>

```python
# df.cov()를 사용해 공분산 행렬 나타내기
# 대각선의 값은 공분산의 값을 나타내지만 a1-a1, a2-a2 .. 자기자신의 관계의 경우 데이터셋의 분산을 의미한다.
df.cov()

# np.cov()를 사용하여 공분산 행렬 나타내기
np.cov(df.T)
```
 <br/>
 >  유사한 연관성이라도 스케일이 큰 변수들은 스케일이 작은 변수들에 비해 높은 공분산 값을 가지게 됩니다.
 >  공분산은 데이터의 스케일에 굉장히 많은 영향을 받습니다. 이를 보완하기 위해 **`상관계수`** 를 구합니다.
 <br/>
 
### 상관계수 (Correlation coeffecient)
* 공분산을 두 변수의 표준편차로 나눠준 값입니다.
> $r_{x, y} = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}$
* 공분산의 스케일을 조정하는 효과가 있습니다.
* 변수의 스케일에 영향을 받지 않습니다.
* -1 에서 1사이의 값을 가집니다. 
* 상관계수가 1이라는 것은 한 변수가 다른 변수에 대해서 완벽한 양의 선형관계를 갖고있다는 것을 의미합니다.
* (이미지 첨부)
* **`df.corr()`** 또는 **`np.corrcoef()`** 를 사용하여 구할 수 있습니다.

```python
# df.corr()를 사용하여 상관계수 나타내기
df.corr()

# np.corrcoef()를 사용하여 상관계수 나타내기
np.corrcoef(df.T)
```
<br/>
<br/>

## 선형대수 (Lenear Algebra)
### Vector Transformation

어떤 벡터A에 행렬T를 곱하면 벡터B로 변환할 수 있는데 되는데 [이때 크기와 방향이 모두 바뀐다.] <br/>
[그러나,] 어떤 특정 벡터V는 T를 곱해도 방향이 바뀌지 않고 크기만 바뀌게 되는데 이것을 **`Eigenvector`** 라고 합니다. <br/>

### a. Eigenvector
* 주어진 transformation에 의해서 [크기만 변하고 방향은 변하지 않는] 벡터입니다. <br/>

### b. Eigenvalue
* Eigenvector의 변화한 크기 값입니다. <br/>
> $T(v) = \lambda v \quad$ where $\ v$ : eigenvector , $\lambda$ : eigenvalue
> $T \cdot v = v' = \lambda \cdot v$
* **`Eigenstuff`** 는 np.linalg.eig()을 사용하여 구할 수 있습니다. <br/>
```python
values, vectors = np.linalg.eig(T)
print("\n Eigenvalues: \n", values)
print("\n Eigenvectors: \n", np.round(vectors, 2))
``` 
 <br/>결과값<br/> 
 
```python
Eigenvalues: 
 [11.  1.]

 Eigenvectors: 
 [[ 0.89 -0.45]
 [ 0.45  0.89]]
```

11에 대응하는 Eigenvector는 0.78, 0.45 즉, 첫 번째 컬럼이고 1에 대응하는 Eigenvector는 -0.45, 0.89 즉, 두 번째 컬럼이다.
<br/>
<br/> 

# 차원축소 (Dimensionality Reduction)
***
정보 손실은 최소화하면서 중요한 변수만 선택할 수 있다? <br/>
현재도 이를 위한 다양한 차원 축소 기술들을 연구 중이라고 합니다. 
<br/> 

### a. Feature selection
* 데이터셋에서 덜 중요한 feature를 제거하는 방법입니다.
* 분석 목적에 적합한 소수의 기존 feature를 선택합니다.
* feature의 해석이 쉽다는 장점이 있습니다.
* feature 간 연관성을 직접 고려해야 합니다.
<br/> 

### b. Feature extraction
* 기존 feature를 조합하여 사용합니다.
* feature 간 상관관계를 고려하여 조합합니다.
  * 원래 변수들의 선형결합으로 이루어집니다.
  * 새로운 feature가 추출(생성)됩니다. 
* feature의 해석이 어렵습니다.
* feature의 수를 많이 줄일 수 있습니다.
<br/> 
<br/> 
> PCA는 차원 축소 방법 중 **`Feature extraction`** 에 속합니다.
<br/> 

# 주성분 분석 (Principal Component Analysis, PCA)
***
* 원래 데이터의 정보(분산)를 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영(Linear Projection)하여 고차원의 데이터를 저차원으로 변환하는 기법입니다.
  * 주성분(PC)은 기존 데이터의 분산을 최대한 보존하도록 데이터를 projection 하는 축입니다.
  * PC의 단위벡터는 데이터 공분산 행렬에 대한 `eigenvector`입니다.
  * eigenvector에 projection한 데이터의 분산이 `eigenvalue`입니다.
* Feature Extraction 방식 중 하나로, 기존 변수의 선형결합으로 새로운 변수(PC)를 생성합니다. 
<br/>
<br/> 

## a. PCA Process
* '선형결합을 한다'는 의미는 데이터가 `numerical`한 데이터여야 한다는 것. 따라서, PCA는 `numeric`데이터에 적용할 수 있다.
* 데이터 표준화 : 각 column에 대해 평균을 빼고 표준편차로 나누어 데이터를 `평균 : 0`, `표준편차 : 1`로 scaling 하는 것.
* scaling한 표준편차를 제곱하면 1이 됩니다. 표준편차를 제곱한 것은 분산입니다. 분산도 1이 됩니다.
* 공분산행렬의 Eigenstuff 구하기. 
  데이터가 최대한 보존되도록 즉, 데이터의 분산이 가장 [크도록] 투영시킬 수 있는 새로운 축을 찾을 것인데, 이 축의 단위벡터가 [공분산행렬의 Eigenvector]이다.
<br/> 
* 데이터를 eigenvetor에 projection 하기
  * PCA는 데이터를 축에 수직으로 projection하는 것인데, eigenvecton는 이 축의 단위벡터입니다.
  * linear projection
  * projection 된 데이터는 기존 데이터의  eigenvector의 내적과 같습니다.
<br/> 
<br/> 

## b. sklearn을 사용하여 PCA 시행

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
```

<br/> 
## Scree Plot
* 주성분의 분산 혹은 분산비율을 표시한 그림입니다.
* 각각의 PC가 얼마나 분산이 큰지, 얼마나 데이터를 보존하는지 알아야만 몇 개의 PC를 선택할 지 구할 수 있습니다. 그 때, 하나의 기준이 되는 것이 Scree Plot 입니다.
* PCA를 몇 차원으로 해야할 지 선택할 때, 하나의 기준이 될 수 있습니다.
* 라인은 누적되는 분산을 보여줍니다.
* 절대적인 것은 아니지만 통장적으로 분산의 비율이 `70~80%` 정도면 무난하다고 표현합니다.

```python
# 분산 비율 
pca.explained_variance_ratio_
```
### PCA는 비지도학습 기법입니다.
### 분산이 크면 정보량을 많이 담고있다고 할 수 있다. 분산은 정보량과 관련이 있다.
