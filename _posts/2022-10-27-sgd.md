---
title: `[`AIB] 미분 그리고 경사 하강법(SGD)
author: yun
date: 2022-10-26
categories: [Blogging, Study, Ai, Summary]
tags: [study, python, sgd]
---

# 미분

# 확률적 경사 하강법(SGD: Stochastric Gradient Descent)
***
신경망 학습의 목표는 **손실함수**의 값을 최대한 낮추는 매개변수(파라미터)를 찾는 것 으로<br/> 
이러한 매개변수의 최적값을 찾는 과정을 **매개변수 최적화**라고 합니다.<br/>

* 머신러닝에서는 오차를 **손실(Loss)** 또는 **비용(Cost)**, **오차(Error)** 라고 하며 <br/>
* 오차를 정의한 함수를 `**손실함수(Loss function)**` 또는 **비용함수(Cost function)**, **오차함수(Error function)** 라고 합니다.<br/>

* **경사하강 알고리즘 과정** <br/>
  1. 경사하강법은 임의의 를 랜덤으로 선택합니다. 즉, random initialization을 실행합니다. <br/>
  2. 반복적으로 파라미터를 업데이트 해가며, 손실함수의 값이 낮아지는 방향으로 진행합니다. <br/>
  3. 이때 기울기는 항상 손실함수 값이 가장 크게 감소하는 방향으로 진행합니다. 그렇기에 경사하강법 알고리즘은 ***기울기의 반대방향***으로 이동합니다. <br/>
  4. 그리고 기울기가 0이 되어 `global minimum`에 도달할 때까지 이동을 합니다. <br/>
<br/>

## 학습률(Lerning Rate)
학습률을 정할 때는 신중해야 합니다. <br/>
학습률이 *지나치게 작다*면 최솟값에 도달하기 위해 *굉장히 많은 연산*이 요구됩니다. <br/>
반대로 학습률이 *지나치게 크다면*  $ \theta $ 가 반대쪽을 오가며 *매우 큰 거리를 이동하게 되어 최솟값에서 점점 멀어지게*  됩니다. <br/>
따라서 **적절한 `Learning Rate`를 설정**하는 것은 매우 중요합니다. <br/>

* Linear regression 은 두 개의 파라미터에 의해서 모양이 형성이 되며 두 개의 파라미터가 되는 것은 `**y절편**`과 `**기울기**` 입니다.
